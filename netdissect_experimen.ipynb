{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHipgdVwLwnG5wpaSbZS/k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thyungkim/NetDissect-Improve-Accuracy/blob/main/netdissect_experimen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Network Dissection (for classifiers)\n",
        "In this notebook, we will examine internal layer representations for a classifier trained to recognize scene categories.\n",
        "\n",
        "Setup matplotlib, torch, and numpy for a high-resolution browser.\n",
        "\n"
      ],
      "metadata": {
        "id": "dJKZYQScJ6WK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "msdUmV_bJTXa"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from importlib import reload\n",
        "import IPython\n",
        "mpl.rcParams['lines.linewidth'] = 0.25\n",
        "mpl.rcParams['axes.spines.top'] = False\n",
        "mpl.rcParams['axes.spines.right'] = False\n",
        "mpl.rcParams['axes.linewidth'] = 0.25"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up experiment directory and settings\n",
        "\n"
      ],
      "metadata": {
        "id": "-2gF_BcTJ_SU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, argparse, os, shutil, inspect, json, numpy, math\n",
        "from easydict import EasyDict\n",
        "import dissect_experiment as experiment\n",
        "\n",
        "# choices are alexnet, vgg16, or resnet152.\n",
        "args = EasyDict(model='vgg16', dataset='places', seg='netpqc', layer='conv5_3', quantile=0.01)\n",
        "resdir = 'results/%s-%s-%s-%s-%s' % (args.model, args.dataset, args.seg, args.layer, int(args.quantile * 1000))\n",
        "def resfile(f):\n",
        "    return os.path.join(resdir, f)"
      ],
      "metadata": {
        "id": "kT8AvuVQJvzd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load classifier model and dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "gz2cbIcHKBax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = experiment.load_model(args)\n",
        "layername = experiment.instrumented_layername(args)\n",
        "model.retain_layer(layername)\n",
        "dataset = experiment.load_dataset(args)\n",
        "upfn = experiment.make_upfn(args, dataset, model, layername)\n",
        "sample_size = len(dataset)\n",
        "percent_level = 1.0 - args.quantile\n",
        "\n",
        "print('Inspecting layer %s of model %s on %s' % (layername, args.model, args.dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mZxS_slJxV7",
        "outputId": "854c3292-cf25-4949-e3cd-e22310bac8e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "upsampling from data_shape (14, 14)\n",
            "Inspecting layer features.conv5_3 of model vgg16 on places\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "wf0jMHpVJyxC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec66d4a9-4714-4021-dff4-9f13741b9f78"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "InstrumentedModel(\n",
              "  (model): VGG(\n",
              "    (features): Sequential(\n",
              "      (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu1_1): ReLU(inplace=True)\n",
              "      (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu1_2): ReLU(inplace=True)\n",
              "      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu2_1): ReLU(inplace=True)\n",
              "      (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu2_2): ReLU(inplace=True)\n",
              "      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu3_1): ReLU(inplace=True)\n",
              "      (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu3_2): ReLU(inplace=True)\n",
              "      (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu3_3): ReLU(inplace=True)\n",
              "      (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu4_1): ReLU(inplace=True)\n",
              "      (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu4_2): ReLU(inplace=True)\n",
              "      (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu4_3): ReLU(inplace=True)\n",
              "      (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu5_1): ReLU(inplace=True)\n",
              "      (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu5_2): ReLU(inplace=True)\n",
              "      (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu5_3): ReLU(inplace=True)\n",
              "      (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "    (classifier): Sequential(\n",
              "      (fc6): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "      (relu6): ReLU(inplace=True)\n",
              "      (drop6): Dropout(p=0.5, inplace=False)\n",
              "      (fc7): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "      (relu7): ReLU(inplace=True)\n",
              "      (drop7): Dropout(p=0.5, inplace=False)\n",
              "      (fc8a): Linear(in_features=4096, out_features=365, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load segmenter, segment labels, classifier labels\n",
        "\n"
      ],
      "metadata": {
        "id": "1sMJ3gjFKDB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Classifier labels\n",
        "from urllib.request import urlopen\n",
        "import renormalize\n",
        "\n",
        "# synset_url = 'http://gandissect.csail.mit.edu/models/categories_places365.txt'\n",
        "# classlabels = [r.split(' ')[0][3:] for r in urlopen(synset_url).read().decode('utf-8').split('\\n')]\n",
        "classlabels = dataset.classes\n",
        "segmodel, seglabels, segcatlabels = experiment.setting.load_segmenter(args.seg)\n",
        "renorm = renormalize.renormalizer(dataset, target='zc')"
      ],
      "metadata": {
        "id": "YdH11GbQJzz9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "5e8dd66c-2e96-4a60-8101-771373e55b24"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-25c7ecc54d00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# classlabels = [r.split(' ')[0][3:] for r in urlopen(synset_url).read().decode('utf-8').split('\\n')]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mclasslabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msegmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseglabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegcatlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_segmenter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mrenorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenormalize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/setting.py\u001b[0m in \u001b[0;36mload_segmenter\u001b[0;34m(segmenter_name)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0msegmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     segmodels.append(segmenter.UnifiedParsingSegmenter(segsizes=[256],\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mall_parts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_parts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             segdiv=('quad' if quad_seg else None)))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'segmenter' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test classifier on some images"
      ],
      "metadata": {
        "id": "7an05ncxKFSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from netdissect import renormalize\n",
        "\n",
        "indices = [200, 755, 709, 423, 60, 100, 110, 120]\n",
        "batch = torch.cat([dataset[i][0][None,...] for i in indices])\n",
        "truth = [classlabels[dataset[i][1]] for i in indices]\n",
        "preds = model(batch.cuda()).max(1)[1]\n",
        "imgs = [renormalize.as_image(t, source=dataset) for t in batch]\n",
        "prednames = [classlabels[p.item()] for p in preds]\n",
        "show([[img, 'pred: ' + pred, 'true: ' + gt] for img, pred, gt in zip(imgs, prednames, truth)])"
      ],
      "metadata": {
        "id": "wtGLpqnmJ1A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "segment single image, and visualize the labels\n",
        "\n"
      ],
      "metadata": {
        "id": "XfAfOrD_KJfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from netdissect import imgviz\n",
        "\n",
        "iv = imgviz.ImageVisualizer(120, source=dataset)\n",
        "seg = segmodel.segment_batch(renorm(batch).cuda(), downsample=4)\n",
        "\n",
        "show([(iv.image(batch[i]), iv.segmentation(seg[i,0]),\n",
        "            iv.segment_key(seg[i,-1], segmodel))\n",
        "            for i in range(len(seg))])"
      ],
      "metadata": {
        "id": "1D2BMHupKL2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualize activations for single layer of single image\n",
        "\n"
      ],
      "metadata": {
        "id": "Wr_gniNrKN32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from netdissect import imgviz\n",
        "\n",
        "acts = model.retained_layer(layername).cpu()\n",
        "ivsmall = imgviz.ImageVisualizer((100, 100), source=dataset)\n",
        "display(show.blocks(\n",
        "    [[[ivsmall.masked_image(batch[0], acts, (0, u), percent_level=0.99)],\n",
        "      [ivsmall.heatmap(acts, (0, u), mode='nearest')]] for u in range(min(acts.shape[1], 12))]\n",
        "))\n",
        "\n",
        "num_units = acts.shape[1]"
      ],
      "metadata": {
        "id": "JDLsq41JKO8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Collect quantile statistics\n",
        "First, unconditional quantiles over the activations. We will upsample them to 56x56 to match with segmentations later."
      ],
      "metadata": {
        "id": "ed7IrdsvKQjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pbar.descnext('rq')\n",
        "def compute_samples(batch, *args):\n",
        "    image_batch = batch.cuda()\n",
        "    _ = model(image_batch)\n",
        "    acts = model.retained_layer(layername)\n",
        "    hacts = upfn(acts)\n",
        "    return hacts.permute(0, 2, 3, 1).contiguous().view(-1, acts.shape[1])\n",
        "rq = tally.tally_quantile(compute_samples, dataset,\n",
        "                          sample_size=sample_size,\n",
        "                          r=8192,\n",
        "                          num_workers=100,\n",
        "                          pin_memory=True,\n",
        "                          cachefile=resfile('rq.npz'))"
      ],
      "metadata": {
        "id": "Nx51aC63KTid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualize Units\n",
        "Collect topk stats first."
      ],
      "metadata": {
        "id": "T0IP6khBKWPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pbar.descnext('topk')\n",
        "def compute_image_max(batch, *args):\n",
        "    image_batch = batch.cuda()\n",
        "    _ = model(image_batch)\n",
        "    acts = model.retained_layer(layername)\n",
        "    acts = acts.view(acts.shape[0], acts.shape[1], -1)\n",
        "    acts = acts.max(2)[0]\n",
        "    return acts\n",
        "topk = tally.tally_topk(compute_image_max, dataset, sample_size=sample_size,\n",
        "        batch_size=50, num_workers=30, pin_memory=True,\n",
        "        cachefile=resfile('topk.npz'))"
      ],
      "metadata": {
        "id": "FB8zQeFHKXV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# single image visualization\n",
        "print(topk.result()[1][10][6], dataset.images[topk.result()[1][10][6]])\n",
        "image_number = topk.result()[1][10][4].item()\n",
        "unit_number = 10\n",
        "iv = imgviz.ImageVisualizer((224, 224), source=dataset, quantiles=rq,\n",
        "        level=rq.quantiles(percent_level))\n",
        "batch = torch.cat([dataset[i][0][None,...] for i in [image_number]])\n",
        "truth = [classlabels[dataset[i][1]] for i in [image_number]]\n",
        "preds = model(batch.cuda()).max(1)[1]\n",
        "imgs = [renormalize.as_image(t, source=dataset) for t in batch]\n",
        "prednames = [classlabels[p.item()] for p in preds]\n",
        "acts = model.retained_layer(layername)\n",
        "show([[img, 'pred: ' + pred, 'true: ' + gt] for img, pred, gt in zip(imgs, prednames, truth)])\n",
        "show([[iv.masked_image(batch[0], acts, (0, unit_number))]])"
      ],
      "metadata": {
        "id": "qsrPxjBCKbLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we just need to run through and visualize the images.\n",
        "\n"
      ],
      "metadata": {
        "id": "h8SlGAI4Kc9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pbar.descnext('unit_images')\n",
        "\n",
        "iv = imgviz.ImageVisualizer((100, 100), source=dataset, quantiles=rq,\n",
        "        level=rq.quantiles(percent_level))\n",
        "def compute_acts(image_batch):\n",
        "    image_batch = image_batch.cuda()\n",
        "    _ = model(image_batch)\n",
        "    acts_batch = model.retained_layer(layername)\n",
        "    return acts_batch\n",
        "unit_images = iv.masked_images_for_topk(\n",
        "        compute_acts, dataset, topk, k=5, num_workers=30, pin_memory=True,\n",
        "        cachefile=resfile('top5images.npz'))"
      ],
      "metadata": {
        "id": "SMaV9j3kKeHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for u in [10, 20, 30, 40, 19, 190]:\n",
        "    print('unit %d' % u)\n",
        "    display(unit_images[u])"
      ],
      "metadata": {
        "id": "XccY19r3Kfg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Label Units\n",
        "Collect 99% quantile stats."
      ],
      "metadata": {
        "id": "v_M19pBeKggG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "level_at_99 = rq.quantiles(percent_level).cuda()[None,:,None,None]\n",
        "# Use the segmodel for segmentations.  With broden, we could use ground truth instead.\n",
        "def compute_conditional_indicator(batch, *args):\n",
        "    image_batch = batch.cuda()\n",
        "    seg = segmodel.segment_batch(renorm(image_batch), downsample=4)\n",
        "    _ = model(image_batch)\n",
        "    acts = model.retained_layer(layername)\n",
        "    hacts = upfn(acts)\n",
        "    iacts = (hacts > level_at_99).float() # indicator\n",
        "    return tally.conditional_samples(iacts, seg)\n",
        "pbar.descnext('condi99')\n",
        "condi99 = tally.tally_conditional_mean(compute_conditional_indicator,\n",
        "        dataset, sample_size=sample_size,\n",
        "        num_workers=3, pin_memory=True,\n",
        "        cachefile=resfile('condi99.npz'))"
      ],
      "metadata": {
        "id": "XFkYoyyOKmjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "iou_99 = tally.iou_from_conditional_indicator_mean(condi99)\n",
        "unit_label_99 = [\n",
        "        (concept.item(), seglabels[concept], segcatlabels[concept], bestiou.item())\n",
        "        for (bestiou, concept) in zip(*iou_99.max(0))]\n",
        "label_list = [labelcat for concept, label, labelcat, iou in unit_label_99 if iou > 0.04]\n",
        "display(IPython.display.SVG(experiment.graph_conceptcatlist(label_list)))\n",
        "len(label_list)"
      ],
      "metadata": {
        "id": "8ELImR5tKoEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show a few units with their labels\n",
        "\n"
      ],
      "metadata": {
        "id": "GbJyQR1SKobY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for u in [10, 20, 30, 40]:\n",
        "    print('unit %d, label %s, iou %.3f' % (u, unit_label_99[u][1], unit_label_99[u][3]))\n",
        "    display(unit_images[u])"
      ],
      "metadata": {
        "id": "8f9Dv4SEKpvH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}